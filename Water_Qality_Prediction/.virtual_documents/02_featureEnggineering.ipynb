import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,  ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from tqdm import tqdm



# Load data
data= pd.read_csv("water_quality_dataset_100k_new.csv")





data.head()


data = data.drop(columns=["Index"])


df=data.copy()


df


df["Target"].value_counts()


target_counts = df["Target"].value_counts()


plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%')
plt.show()


#saparate numarical values
num_features = df.select_dtypes(include=[np.number]).columns.tolist()
num_features.remove("Target")
num_features


#saparate categorical features
cat_features = df.select_dtypes(include=[object]).columns.tolist()

cat_features


#write /plot histogram for numarical feature
df[num_features].hist(bins=50, figsize=(20,15))
plt.show()


#box plt
plt.figure(figsize=(20,15))
for i, column in enumerate(df[num_features]):
    plt.subplot(6, 6, i + 1)
    sns.boxplot(data=df[num_features], x=column)
    plt.title(column)
plt.suptitle("Boxplots of Numerical Features", fontsize=16)
plt.tight_layout()
plt.show()


#categorical plt
plt.figure(figsize=(10,4))
for column in cat_features:
    plt.figure(figsize=(10,4))
    sns.countplot(data=df, x=column)
    plt.title(f"count plot of {column} with target")
    plt.show()


print("Correlation matrix of num features:")
plt.figure(figsize=(12,10))
corr_matrix = df[num_features+["Target"]].corr()
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True, cbar_kws={"shrink": .8})
plt.title("Correlation matrix of numerical features")
plt.show()


#data clean
df.isna().sum()




df.shape


missing = df.isna().sum(axis=1)/df.shape[1]*100
missing.value_counts()


#detect the missing value
missing_value = df.isnull().sum()
missing_value


#visualize missing value
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values in Each Column')

plt.show()


missing_value = missing_value[missing_value > 0]
print(missing_value)
df.dropna()


df.isna().sum().sort_values()


#impute missing values for numerical features with the mean
num_features = df.select_dtypes(include=[np.number]).columns.tolist()
df[num_features] = df[num_features].fillna(df[num_features].mean())
#verifying if missing value is imputed
print(df.isna().sum().sort_values())


#impute missing values for categorical features with the mode
cat_features = df.select_dtypes(include=[object]).columns.tolist()
df[cat_features] = df[cat_features].fillna(df[cat_features].mode().iloc[0])
#verifying if missing value is imputed
print(df.isna().sum())


#convert categorical data into numerical
num_features = df.select_dtypes(include=['int64','float64']).columns.tolist()
cat_features=df.select_dtypes(include=['object','category']).columns.tolist()


#exclude the target and non feature columns if present
non_feature_cols = ['Target','Month','Day','Time of Day',]
num_features = [col for col in num_features if col not in non_feature_cols]
cat_features = [col for col in cat_features if col not in non_feature_cols]
cat_features


#normalize of standerize numerical features
numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])
#convert categorical features
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])
#combain transformer into a preprocessor
preprocessor = ColumnTransformer(transformers=[
    ("num", numerical_transformer, num_features),
    ("cat", categorical_transformer, cat_features)
])
#apply the transformations to the dataframe
df_preprocessed = preprocessor.fit_transform(df)
#which sahpe of transform data
print("Shape of preprocessed data:", df_preprocessed.shape)


df_preprocessed



#get feature names
numerical_features = num_features
categorical_features = preprocessor.named_transformers_["cat"]["onehot"].get_feature_names_out(cat_features)
all_features = numerical_features + list(categorical_features)
print(all_features)


#convert data frame
df_preprocessed = pd.DataFrame(df_preprocessed,columns=all_features)
df_preprocessed[non_feature_cols]=df[non_feature_cols].reset_index(drop=True)
df_preprocessed


df_preprocessed.to_csv("water_quality_dataset_100k_preprocessed.csv", index=False)


#features Engineering
df=pd.read_csv("water_quality_dataset_100k_preprocessed.csv")


df


# Select only numerical columns for correlation matrix
numerical_columns_for_corr = df_preprocessed.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Visualize distributions of numerical features after transformation
df_preprocessed[numerical_columns_for_corr].hist(figsize=(20, 15), bins=15,)
plt.suptitle('Histograms of Numerical Features After Transformation', fontsize=16)
plt.show()


plt.figure(figsize=(15,10))
sns.boxplot(data=df.drop(['Time of Day','Day'], axis=1),orient='h')
plt.show()


plt.figure(figsize=(14,12))
df.drop(['Time of Day','Day'], axis=1).boxplot()
plt.title("Boxplot of numerical features after transformation",fontsize=16)
plt.xticks(rotation=90)
plt.show()


plt.figure(figsize=(14,12))
correlation_matrix = df[numerical_features_corr].corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix of Numerical Features", fontsize=16)
plt.show()


num_features=df.select_dtypes(include=[np.number]).columns.tolist()
num_features.remove('Target')



#feature Engineering
df['water_tem_to_Air_ratio']=df['Water Temperature']/df['Air Temperature']
df['total_metals']=df[['Iron', 'Lead', 'Copper', 'Zinc', 'Manganese']].sum(axis=1)
num_features.extend(['water_tem_to_Air_ratio', 'total_metals'])
num_features


cat_features



# Apply transformations
X = df.drop('Target', axis=1)
y = df['Target']
X_transformed_df=X[num_features].copy()
print("\nTransformed Features Head:")
X_transformed_df.head()



X_transformed_df.columns


X_transformed_df.shape


# write your code
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed_df, y, test_size=0.2, random_state=42)



# write your code
model_rf=RandomForestClassifier(n_estimators=10, random_state=42)
model_rf.fit(X_train, y_train)


# Evaluate the model

y_pred = model_rf.predict(X_test)
    
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

results= {
    'accuracy': accuracy,
    'precision': precision,
    'recall': recall,
    'f1': f1}

results


# create function to get results to avoid repetition
def get_results(y_test, y_pred):
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results= {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1}
    return results


get_results(y_test, y_pred)


# Define models to compare
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=10, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42)
}

# Train and evaluate each model with progress tracking using tqdm
results = {}
for name, model in tqdm(models.items(), desc="Training and Evaluating Models"):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = get_results(y_test, y_pred)


# Display results
results_df = pd.DataFrame(results).T
print("\nModel Performance Comparison:")
results_df


# Plot results for visual comparison
results_df.plot(kind='bar', figsize=(12, 8))
plt.title('Model Performance Comparison')
plt.ylabel('Score')
plt.xlabel('Model')
plt.grid()
plt.show()



# Choose the best model based on Accuracy  you can taste the model withe the help of 
#any metrics(accuracy,f1,precision,recall)
best_model_name = results_df['accuracy'].idxmax()
best_model = models[best_model_name]
print(f"\nBest Model: {best_model_name}")


# Generate and plot the confusion matrix
def plot_confusion_matrix (model, X_test, model_name):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix for {model_name} Model")
    plt.show()


plot_confusion_matrix (model_rf, X_test, 'Random Forest')


# Step 5: Model Interpretation and Insights
# ==============================

# Interpret the best model to understand which factors most influence water quality
if hasattr(best_model, 'feature_importances_'):
    # feature_importance = pd.Series(best_model.feature_importances_, index=numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)))
    feature_importance = pd.Series(best_model.feature_importances_, index=num_features )
    feature_importance.sort_values(ascending=False, inplace=True)
    
    plt.figure(figsize=(10, 6))
    feature_importance.plot(kind='bar')
    plt.title(f'Feature Importance in Best Model - {best_model}')
    plt.show()

important_features = feature_importance.head(10).index if 'feature_importances_' in dir(best_model) else num_features
print(f'\nTop 5 important features: {important_features}')



